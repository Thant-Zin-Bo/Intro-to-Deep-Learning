{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN Architecture\n",
    "\n",
    "This code snippet demonstrates how to build a Convolutional Neural Network (CNN) architecture using the `keras` library in Python. The model consists of multiple convolutional layers, max-pooling layers, a flattening layer, and fully connected layers.\n",
    "\n",
    "## Code Explanation\n",
    "\n",
    "### 1. Input Layer\n",
    "First, we create an input layer specifying the shape of the input data.\n",
    "\n",
    "```python\n",
    "inputs = Input(shape=input_shape)\n",
    "```\n",
    "### 2. Define the Model Architecture\n",
    "We use a Sequential model to define the architecture of the CNN.\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "### 3. Convolutional and Max-Pooling Layers\n",
    "\n",
    "#### Convolutional Layer 1\n",
    "- **Conv2D**: 32 filters, kernel size of 5x5, activation function `relu`.\n",
    "- **MaxPooling2D**: Pool size of 2x2.\n",
    "```python\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "```\n",
    "#### Convolutional Layer 2\n",
    "- **Conv2D**: 64 filters, kernel size of 5x5, activation function `relu`.\n",
    "- **MaxPooling2D**: Pool size of 2x2.\n",
    "```python\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "```\n",
    "\n",
    "#### Convolutional Layer 3\n",
    "- **Conv2D**: 128 filters, kernel size of 5x5, activation function `relu`.\n",
    "- **MaxPooling2D**: Pool size of 2x2.\n",
    "```python\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "```\n",
    "\n",
    "### 4. Flattening and Fully Connected Layers\n",
    "#### Flattening Layer \n",
    "The flattening layer converts the 2D matrix data to a 1D vector.\n",
    "\n",
    "```python\n",
    "model.add(Flatten())\n",
    "```\n",
    "#### Fully Connected Layer\n",
    "- **Dense**:  512 units, activation function `relu`.\n",
    "- **Output layer**: 10 units (assuming 10 classes for classification), activation function `softmax`.\n",
    "```python\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "### 5. Build the Model\n",
    "\n",
    "Finally, we build the model by specifying the inputs and outputs.\n",
    "```python\n",
    "model = Model(inputs=inputs, outputs=model(inputs))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from tensorflow.keras.models import Sequential, Model # type: ignore\n",
    "from tensorflow.keras.utils import plot_model # type: ignore\n",
    "from tensorflow.keras.layers import Input,Conv2D, MaxPooling2D, Flatten, Dense, Dropout # type: ignore\n",
    "\n",
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape):    \n",
    "    \n",
    "    # Create an Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Define the rest of the model architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    model.add(Conv2D(128, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flattening Layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully Connected Layer 1\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes for classification\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs=inputs, outputs=model(inputs))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (64, 64, 3)  # Specify the input shape\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet, short for Residual Network, is a type of deep neural network architecture introduced by Kaiming He et al. in 2015. Deep neural networks often suffer from the vanishing gradient problem, where gradients become progressively smaller as they are backpropagated from the output layer to the input layer. This means that earlier layers (closer to the input) receive extremely small updates, slowing down their learning or making it nearly negligible. This issue is common in deep networks and hinders their training.\n",
    "\n",
    "Similarly, deep networks can also face the exploding gradient problem, where gradients grow exponentially during backpropagation, making the model unstable. ResNet addresses both vanishing and exploding gradient problems by introducing shortcut connections, or residual connections, that allow the gradient to bypass one or more layers as shown in the figure.\n",
    "\n",
    "The core idea of ResNet is the use of residual blocks, which consist of two or three layers with a direct (shortcut) connection that skips one or more layers. These shortcut connections help preserve the flow of gradients, making it feasible to train very deep networks.\n",
    "\n",
    "1. Basic Residual Block: Used in ResNet-18 and ResNet-34, each block has 2 layers, typically 3x3 convolutional layers.\n",
    "2. Bottleneck Residual Block: Used in ResNet-50 and ResNet-101, each block has 3 layers: a 1x1 convolutional layer for dimensionality reduction, a 3x3 convolutional layer, and another 1x1 convolutional layer for restoring the dimensions. In bottleneck design, the 1Ã—1 convolution layers are added at the beginning and end of each block to reduce and then restore the number of channels, respectively, as shown in the figure (right).\n",
    "\n",
    "Shallower networks like ResNet-18 and ResNet-34 are faster and less computationally intensive, making them suitable for tasks where computational resources are limited. Deeper networks like ResNet-50 and ResNet-101 can capture more complex patterns, making them better suited for more challenging tasks but at the cost of higher computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet from Scratch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "\n",
    "def resnet_block(inputs, filters, kernel_size, strides):\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if strides > 1:\n",
    "        inputs = layers.Conv2D(filters, 1, strides=strides, padding='same')(inputs)\n",
    "    \n",
    "    x = layers.Add()([x, inputs])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, strides=1)\n",
    "    x = resnet_block(x, 64, 3, strides=1)\n",
    "    x = resnet_block(x, 64, 3, strides=1)\n",
    "    \n",
    "    x = resnet_block(x, 128, 3, strides=2)\n",
    "    x = resnet_block(x, 128, 3, strides=1)\n",
    "    x = resnet_block(x, 128, 3, strides=1)\n",
    "    x = resnet_block(x, 128, 3, strides=1)\n",
    "    \n",
    "    x = resnet_block(x, 256, 3, strides=2)\n",
    "    x = resnet_block(x, 256, 3, strides=1)\n",
    "    x = resnet_block(x, 256, 3, strides=1)\n",
    "    x = resnet_block(x, 256, 3, strides=1)\n",
    "    x = resnet_block(x, 256, 3, strides=1)\n",
    "    x = resnet_block(x, 256, 3, strides=1)\n",
    "    \n",
    "    x = resnet_block(x, 512, 3, strides=2)\n",
    "    x = resnet_block(x, 512, 3, strides=1)\n",
    "    x = resnet_block(x, 512, 3, strides=1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 1000\n",
    "resnet_model = build_resnet(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "\n",
    "def efficientnet_block(inputs, filters, kernel_size, strides, expand_ratio):\n",
    "    channel_axis = 1 if tf.keras.backend.image_data_format() == 'channels_first' else -1\n",
    "    input_shape = tf.keras.backend.int_shape(inputs)\n",
    "    input_filters = input_shape[channel_axis]\n",
    "    expanded_filters = input_filters * expand_ratio\n",
    "    \n",
    "    x = inputs\n",
    "    if expand_ratio != 1:\n",
    "        x = layers.Conv2D(expanded_filters, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('swish')(x)\n",
    "    \n",
    "    x = layers.DepthwiseConv2D(kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if strides == 1 and input_filters == filters:\n",
    "        x = layers.Add()([x, inputs])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_efficientnet(input_shape, num_classes, width_coefficient, depth_coefficient, dropout_rate):\n",
    "    channel_axis = 1 if tf.keras.backend.image_data_format() == 'channels_first' else -1\n",
    "    \n",
    "    # Define the number of filters for each block\n",
    "    block_filters = {\n",
    "        'b0': [32, 16, 24, 40, 80, 112, 192, 320],\n",
    "        'b1': [32, 16, 24, 40, 80, 112, 192, 320],\n",
    "        'b2': [32, 16, 24, 48, 88, 120, 208, 352],\n",
    "        'b3': [40, 24, 32, 64, 112, 160, 272, 464],\n",
    "        'b4': [48, 24, 32, 64, 112, 160, 272, 464],\n",
    "        'b5': [48, 24, 40, 80, 160, 224, 384, 640],\n",
    "        'b6': [56, 32, 40, 80, 160, 224, 384, 640],\n",
    "        'b7': [64, 32, 48, 96, 192, 256, 448, 768]\n",
    "    }\n",
    "    \n",
    "    # Define the number of layers for each block\n",
    "    block_layers = {\n",
    "        'b0': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'b1': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'b2': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'b3': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'b4': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'b5': [1, 2, 3, 4, 4, 6, 1],\n",
    "        'b6': [1, 2, 3, 4, 4, 6, 1],\n",
    "        'b7': [1, 2, 3, 4, 4, 6, 1]\n",
    "    }\n",
    "    \n",
    "    # Calculate the number of filters and layers based on the width and depth coefficients\n",
    "    num_filters = [int(x * width_coefficient) for x in block_filters['b0']]\n",
    "    num_layers = [int(x * depth_coefficient) for x in block_layers['b0']]\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Stem convolutional layer\n",
    "    x = layers.Conv2D(num_filters[0], 3, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    # Building blocks\n",
    "    for i in range(7):\n",
    "        for j in range(num_layers[i]):\n",
    "            strides = 2 if j == 0 and i != 0 else 1\n",
    "            x = efficientnet_block(x, num_filters[i+1], 3, strides, expand_ratio=1)\n",
    "    \n",
    "    # Head convolutional layer\n",
    "    x = layers.Conv2D(num_filters[-1], 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    # Global average pooling and dropout\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 1000\n",
    "width_coefficient = 1.0\n",
    "depth_coefficient = 1.0\n",
    "dropout_rate = 0.2\n",
    "\n",
    "efficientnet_model = build_efficientnet(input_shape, num_classes, width_coefficient, depth_coefficient, dropout_rate)\n",
    "efficientnet_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
